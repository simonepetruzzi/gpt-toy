name: wikipedia
data_dir: /path/to/wikipedia/dataset
preprocessing:
  tokenizer: bert-base-uncased
  vocab_size: 30522
  block_size: 512

data_split:
  train_size: 0.8
  val_size: 0.1
  test_size: 0.1

batch_size: 16
shuffle: true
num_workers: 4
pin_memory: true